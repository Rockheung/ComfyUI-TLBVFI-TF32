# ComfyUI-Frame-Interpolation ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° í”„ë ˆì„ ë³´ê°„ ì „ëµ ë¶„ì„

**ë¶„ì„ ëŒ€ìƒ**: [Fannovel16/ComfyUI-Frame-Interpolation](https://github.com/Fannovel16/ComfyUI-Frame-Interpolation)
**ë¶„ì„ ì¼ì**: 2025-10-15
**ë¶„ì„ì**: Claude Code + Rockheung

---

## ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [RIFE VFI êµ¬í˜„ ë¶„ì„](#rife-vfi-êµ¬í˜„-ë¶„ì„)
3. [FILM VFI êµ¬í˜„ ë¶„ì„](#film-vfi-êµ¬í˜„-ë¶„ì„)
4. [ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ ë¹„êµ](#ë©”ëª¨ë¦¬-ê´€ë¦¬-ì „ëµ-ë¹„êµ)
5. [Best Practices ì¶”ì¶œ](#best-practices-ì¶”ì¶œ)
6. [TLBVFI êµ¬í˜„ê³¼ì˜ ë¹„êµ](#tlbvfi-êµ¬í˜„ê³¼ì˜-ë¹„êµ)
7. [ê¶Œì¥ì‚¬í•­](#ê¶Œì¥ì‚¬í•­)

---

## ê°œìš”

ComfyUI-Frame-Interpolationì€ 10ê°€ì§€ ì´ìƒì˜ VFI ëª¨ë¸(RIFE, FILM, AMT, FLAVR ë“±)ì„ ì œê³µí•˜ëŠ” ì¢…í•© í”„ë ˆì„ ë³´ê°„ í™•ì¥ì…ë‹ˆë‹¤. ì´ ë¶„ì„ì—ì„œëŠ” íŠ¹íˆ RIFEì™€ FILMì˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° í”„ë ˆì„ ì²˜ë¦¬ ì „ëµì— ì§‘ì¤‘í•©ë‹ˆë‹¤.

### ì£¼ìš” ë°œê²¬ì‚¬í•­

- **ëª¨ë¸ ìºì‹± ì—†ìŒ**: ë§¤ ì‹¤í–‰ë§ˆë‹¤ ëª¨ë¸ì„ ìƒˆë¡œ ë¡œë“œ
- **ì£¼ê¸°ì  ìºì‹œ í´ë¦¬ì–´**: ê¸°ë³¸ 10í”„ë ˆì„ë§ˆë‹¤ CUDA ìºì‹œ ì •ë¦¬
- **CPU ê¸°ë°˜ ì¶œë ¥ ëˆ„ì **: GPUê°€ ì•„ë‹Œ CPUì— ê²°ê³¼ ëˆ„ì 
- **ComfyUI í†µí•©**: `comfy.model_management`ì˜ `soft_empty_cache()` í™œìš©

---

## RIFE VFI êµ¬í˜„ ë¶„ì„

### íŒŒì¼ ìœ„ì¹˜
- **ë…¸ë“œ êµ¬í˜„**: `/vfi_models/rife/__init__.py`
- **ì•„í‚¤í…ì²˜**: `/vfi_models/rife/rife_arch.py`
- **ê³µí†µ ìœ í‹¸ë¦¬í‹°**: `/vfi_utils.py`

### ëª¨ë¸ ë¡œë”© ì „ëµ

```python
# vfi_models/rife/__init__.py:89-94
from .rife_arch import IFNet
model_path = load_file_from_github_release(MODEL_TYPE, ckpt_name)
arch_ver = CKPT_NAME_VER_DICT[ckpt_name]
interpolation_model = IFNet(arch_ver=arch_ver)
interpolation_model.load_state_dict(torch.load(model_path))
interpolation_model.eval().to(get_torch_device())
```

**íŠ¹ì§•**:
- âœ… **ë§¤ ì‹¤í–‰ë§ˆë‹¤ ëª¨ë¸ ë¡œë“œ**: ìºì‹± ì—†ìŒ
- âœ… **ì¦‰ì‹œ GPUë¡œ ì´ë™**: `get_torch_device()`ë¡œ ìë™ ë””ë°”ì´ìŠ¤ ì„ íƒ
- âœ… **eval ëª¨ë“œ ì„¤ì •**: ì¶”ë¡  ìµœì í™”
- âŒ **ì¬ì‚¬ìš© ì—†ìŒ**: ë‹¤ìŒ ì‹¤í–‰ ì‹œ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ

### í”„ë ˆì„ ì²˜ë¦¬ ë£¨í”„

RIFEëŠ” `generic_frame_loop()`ë¥¼ ì‚¬ìš©:

```python
# vfi_utils.py:124-207 (_generic_frame_loop í•¨ìˆ˜)
def _generic_frame_loop(
        frames,
        clear_cache_after_n_frames,
        multiplier,
        return_middle_frame_function,
        *args,
        use_timestep=True,
        dtype=torch.float16):

    # CPUì— ì¶œë ¥ í”„ë ˆì„ ì‚¬ì „ í• ë‹¹
    output_frames = torch.zeros(multiplier*frames.shape[0], *frames.shape[1:],
                                 dtype=dtype, device="cpu")
    out_len = 0
    number_of_frames_processed_since_last_cleared_cuda_cache = 0

    for frame_itr in range(len(frames) - 1):
        frame0 = frames[frame_itr:frame_itr+1]
        output_frames[out_len] = frame0  # ì²« í”„ë ˆì„ ë³µì‚¬
        out_len += 1

        frame0 = frame0.to(dtype=torch.float32)
        frame1 = frames[frame_itr+1:frame_itr+2].to(dtype=torch.float32)

        # ì¤‘ê°„ í”„ë ˆì„ ìƒì„±
        for middle_i in range(1, multiplier):
            timestep = middle_i/multiplier

            middle_frame = return_middle_frame_function(
                frame0.to(DEVICE),
                frame1.to(DEVICE),
                timestep,
                *args
            ).detach().cpu()  # ì¦‰ì‹œ CPUë¡œ ì´ë™

            middle_frame_batches.append(middle_frame.to(dtype=dtype))

        # ì¶œë ¥ì— ë³µì‚¬
        for middle_frame in middle_frame_batches:
            output_frames[out_len] = middle_frame
            out_len += 1

        number_of_frames_processed_since_last_cleared_cuda_cache += 1

        # ì£¼ê¸°ì  ìºì‹œ í´ë¦¬ì–´
        if number_of_frames_processed_since_last_cleared_cuda_cache >= clear_cache_after_n_frames:
            print("Comfy-VFI: Clearing cache...", end=' ')
            soft_empty_cache()
            number_of_frames_processed_since_last_cleared_cuda_cache = 0
            print("Done cache clearing")

        gc.collect()

    # ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ê°€
    output_frames[out_len] = frames[-1:]
    out_len += 1

    # ìµœì¢… ìºì‹œ í´ë¦¬ì–´
    soft_empty_cache()

    return output_frames[:out_len]
```

### ë©”ëª¨ë¦¬ ê´€ë¦¬ í•µì‹¬ ì „ëµ

1. **ì‚¬ì „ í• ë‹¹ CPU í…ì„œ**
   ```python
   output_frames = torch.zeros(multiplier*frames.shape[0], *frames.shape[1:],
                                dtype=dtype, device="cpu")
   ```
   - ëª¨ë“  ì¶œë ¥ì„ CPU ë©”ëª¨ë¦¬ì— ë¯¸ë¦¬ í• ë‹¹
   - GPU VRAM ì‚¬ìš© ìµœì†Œí™”

2. **ì¦‰ì‹œ CPU ì „ì†¡**
   ```python
   middle_frame = return_middle_frame_function(...).detach().cpu()
   ```
   - ìƒì„±ëœ í”„ë ˆì„ì„ ì¦‰ì‹œ CPUë¡œ ì´ë™
   - `.detach()`ë¡œ gradient ì¶”ì  ì œê±°

3. **ì£¼ê¸°ì  ìºì‹œ í´ë¦¬ì–´**
   ```python
   if processed_frames >= clear_cache_after_n_frames:
       soft_empty_cache()  # ComfyUIì˜ ìŠ¤ë§ˆíŠ¸ ìºì‹œ í´ë¦¬ì–´
       gc.collect()         # Python GC
   ```
   - ê¸°ë³¸ 10í”„ë ˆì„ë§ˆë‹¤
   - ì‚¬ìš©ì ì„¤ì • ê°€ëŠ¥ (`clear_cache_after_n_frames` íŒŒë¼ë¯¸í„°)

4. **ìµœì¢… ì •ë¦¬**
   ```python
   soft_empty_cache()  # ì‘ì—… ì™„ë£Œ í›„ ì „ì²´ ì •ë¦¬
   ```

---

## FILM VFI êµ¬í˜„ ë¶„ì„

### íŒŒì¼ ìœ„ì¹˜
- **ë…¸ë“œ êµ¬í˜„**: `/vfi_models/film/__init__.py`
- **ì•„í‚¤í…ì²˜**: TorchScript ëª¨ë¸ (`torch.jit.load`)

### ëª¨ë¸ ë¡œë”© ì „ëµ

```python
# vfi_models/film/__init__.py:73-76
model_path = load_file_from_github_release(MODEL_TYPE, ckpt_name)
model = torch.jit.load(model_path, map_location='cpu')
model.eval()
model = model.to(DEVICE)
```

**íŠ¹ì§•**:
- âœ… **TorchScript ëª¨ë¸**: JIT ì»´íŒŒì¼ëœ ëª¨ë¸ ì‚¬ìš©
- âœ… **CPU ë¨¼ì € ë¡œë“œ**: `map_location='cpu'`ë¡œ ì•ˆì „í•˜ê²Œ ë¡œë“œ
- âœ… **ëª…ì‹œì  ë””ë°”ì´ìŠ¤ ì´ë™**: ì´í›„ `to(DEVICE)`
- âŒ **ìºì‹± ì—†ìŒ**: RIFEì™€ ë™ì¼í•˜ê²Œ ë§¤ë²ˆ ë¡œë“œ

### í”„ë ˆì„ ì²˜ë¦¬ ë£¨í”„

FILMì€ **ë…ìì ì¸ ì²˜ë¦¬ ë£¨í”„** ì‚¬ìš©:

```python
# vfi_models/film/__init__.py:79-113
frames = preprocess_frames(frames)
number_of_frames_processed_since_last_cleared_cuda_cache = 0
output_frames = []  # ë¦¬ìŠ¤íŠ¸ë¡œ ë™ì  ëˆ„ì 

for frame_itr in range(len(frames) - 1):
    # GPUë¡œ í”„ë ˆì„ ì „ì†¡
    frame_0 = frames[frame_itr:frame_itr+1].to(DEVICE).float()
    frame_1 = frames[frame_itr+1:frame_itr+2].to(DEVICE).float()

    # FILMì˜ ì¬ê·€ì  inference
    result = inference(model, frame_0, frame_1, multipliers[frame_itr] - 1)

    # CPUë¡œ ì¦‰ì‹œ ì´ë™ í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    output_frames.extend([
        frame.detach().cpu().to(dtype=dtype)
        for frame in result[:-1]
    ])

    number_of_frames_processed_since_last_cleared_cuda_cache += 1

    # ì£¼ê¸°ì  ìºì‹œ í´ë¦¬ì–´
    if number_of_frames_processed_since_last_cleared_cuda_cache >= clear_cache_after_n_frames:
        print("Comfy-VFI: Clearing cache...", end = ' ')
        soft_empty_cache()
        number_of_frames_processed_since_last_cleared_cuda_cache = 0
        print("Done cache clearing")

    gc.collect()

output_frames.append(frames[-1:].to(dtype=dtype))
output_frames = [frame.cpu() for frame in output_frames]  # ìµœì¢… CPU í™•ì¸
out = torch.cat(output_frames, dim=0)  # í•œë²ˆì— concat

# ìµœì¢… ìºì‹œ í´ë¦¬ì–´
soft_empty_cache()
return (postprocess_frames(out), )
```

### FILMì˜ íŠ¹ìˆ˜í•œ Inference ì „ëµ

```python
# vfi_models/film/__init__.py:12-42
def inference(model, img_batch_1, img_batch_2, inter_frames):
    results = [img_batch_1, img_batch_2]
    idxes = [0, inter_frames + 1]
    remains = list(range(1, inter_frames + 1))
    splits = torch.linspace(0, 1, inter_frames + 2)

    for _ in range(len(remains)):
        # ê±°ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í”„ë ˆì„ ì„ íƒ
        starts = splits[idxes[:-1]]
        ends = splits[idxes[1:]]
        distances = ((splits[None, remains] - starts[:, None]) /
                     (ends[:, None] - starts[:, None]) - .5).abs()

        matrix = torch.argmin(distances).item()
        start_i, step = np.unravel_index(matrix, distances.shape)
        end_i = start_i + 1

        x0 = results[start_i].to(DEVICE)
        x1 = results[end_i].to(DEVICE)
        dt = x0.new_full((1, 1), (splits[remains[step]] - splits[idxes[start_i]])) /
             (splits[idxes[end_i]] - splits[idxes[start_i]])

        with torch.no_grad():
            prediction = model(x0, x1, dt)

        insert_position = bisect.bisect_left(idxes, remains[step])
        idxes.insert(insert_position, remains[step])
        results.insert(insert_position, prediction.clamp(0, 1).float())
        del remains[step]

    return [tensor.flip(0) for tensor in results]
```

**íŠ¹ì§•**:
- ğŸ¯ **ì ì‘í˜• ìƒ˜í”Œë§**: ì‹œê°„ì ìœ¼ë¡œ ê· ë“±í•˜ê²Œ ë¶„ë°°ëœ í”„ë ˆì„ ìƒì„±
- ğŸ”„ **ì¬ê·€ì  ë³´ê°„**: ì´ë¯¸ ìƒì„±ëœ í”„ë ˆì„ ì‚¬ì´ì— ìƒˆ í”„ë ˆì„ ì‚½ì…
- ğŸ“Š **ë™ì  íƒ€ì„ìŠ¤í…**: í”„ë ˆì„ ê°„ ê±°ë¦¬ ê¸°ë°˜ìœ¼ë¡œ timestep ê³„ì‚°
- ğŸ’¾ **ë©”ëª¨ë¦¬ íš¨ìœ¨**: í•œ ë²ˆì— 2ê°œ í”„ë ˆì„ë§Œ GPUì— ë¡œë“œ

---

## ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ ë¹„êµ

### ê³µí†µ ì „ëµ

| ì „ëµ | RIFE | FILM | ì„¤ëª… |
|------|------|------|------|
| **ëª¨ë¸ ìºì‹±** | âŒ | âŒ | ë§¤ ì‹¤í–‰ë§ˆë‹¤ ëª¨ë¸ ì¬ë¡œë“œ |
| **ì£¼ê¸°ì  ìºì‹œ í´ë¦¬ì–´** | âœ… | âœ… | ê¸°ë³¸ 10í”„ë ˆì„ë§ˆë‹¤ `soft_empty_cache()` |
| **CPU ì¶œë ¥ ëˆ„ì ** | âœ… | âœ… | GPU ëŒ€ì‹  CPU ë©”ëª¨ë¦¬ì— ê²°ê³¼ ì €ì¥ |
| **gc.collect()** | âœ… | âœ… | Python ê°€ë¹„ì§€ ì»¬ë ‰í„° í˜¸ì¶œ |
| **ìµœì¢… ì •ë¦¬** | âœ… | âœ… | ì™„ë£Œ í›„ `soft_empty_cache()` |
| **detach() ì‚¬ìš©** | âœ… | âœ… | gradient ì¶”ì  ì œê±° |

### ì°¨ì´ì 

| ì¸¡ë©´ | RIFE | FILM |
|------|------|------|
| **ì¶œë ¥ í• ë‹¹** | ì‚¬ì „ í• ë‹¹ (`torch.zeros`) | ë™ì  ë¦¬ìŠ¤íŠ¸ (`list.extend`) |
| **ëª¨ë¸ íƒ€ì…** | PyTorch í‘œì¤€ ëª¨ë¸ | TorchScript (JIT) |
| **í”„ë ˆì„ ì²˜ë¦¬** | ìˆœì°¨ì  íƒ€ì„ìŠ¤í… | ì ì‘í˜• ì¬ê·€ì  ìƒ˜í”Œë§ |
| **ê³µí†µ ìœ í‹¸ë¦¬í‹°** | `generic_frame_loop()` ì‚¬ìš© | ë…ë¦½ì ì¸ ë£¨í”„ êµ¬í˜„ |
| **dtype** | `float16` (ë©”ëª¨ë¦¬ ì ˆì•½) | `float32` (ì •í™•ë„ ìš°ì„ ) |

### ComfyUI í†µí•©

ë‘˜ ë‹¤ **ComfyUIì˜ `model_management` ëª¨ë“ˆ**ì„ í™œìš©:

```python
from comfy.model_management import soft_empty_cache, get_torch_device

DEVICE = get_torch_device()  # ìë™ ë””ë°”ì´ìŠ¤ ì„ íƒ

# ì£¼ê¸°ì  ìºì‹œ í´ë¦¬ì–´
soft_empty_cache()  # ComfyUIì˜ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê´€ë¦¬
```

**`soft_empty_cache()`ì˜ ì¥ì **:
- ComfyUIì˜ ë‹¤ë¥¸ ë…¸ë“œë“¤ê³¼ ë©”ëª¨ë¦¬ í˜‘ìƒ
- ë‹¨ìˆœ `torch.cuda.empty_cache()`ë³´ë‹¤ ìŠ¤ë§ˆíŠ¸
- ë‹¤ë¥¸ ì‹¤í–‰ ì¤‘ì¸ ì›Œí¬í”Œë¡œìš° ê³ ë ¤

---

## Best Practices ì¶”ì¶œ

### 1. ë©”ëª¨ë¦¬ ê´€ë¦¬

#### âœ… DO: CPU ê¸°ë°˜ ì¶œë ¥ ëˆ„ì 
```python
# GPU ë©”ëª¨ë¦¬ ì••ë°• ë°©ì§€
output_frames = torch.zeros(..., device="cpu")

# ì²˜ë¦¬ í›„ ì¦‰ì‹œ CPUë¡œ ì´ë™
result = model(...).detach().cpu()
```

#### âœ… DO: ì£¼ê¸°ì  ìºì‹œ í´ë¦¬ì–´
```python
if processed_count >= clear_threshold:
    soft_empty_cache()  # ComfyUI í†µí•©
    gc.collect()        # Python GC
    processed_count = 0
```

#### âœ… DO: detach() ì‚¬ìš©
```python
# gradient ì¶”ì  ì œê±°ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
output = model(...).detach()
```

#### âŒ DON'T: GPUì— ëª¨ë“  ì¶œë ¥ ëˆ„ì 
```python
# ë‚˜ìœ ì˜ˆ: OOM ìœ„í—˜
output_frames = torch.zeros(..., device="cuda")  # âŒ
```

### 2. ë””ë°”ì´ìŠ¤ ê´€ë¦¬

#### âœ… DO: ComfyUIì˜ ë””ë°”ì´ìŠ¤ ì„ íƒ í™œìš©
```python
from comfy.model_management import get_torch_device
DEVICE = get_torch_device()  # ìë™ ì„ íƒ
```

#### âœ… DO: ëª…ì‹œì  ì „ì†¡
```python
frame = frame.to(DEVICE)  # ëª…í™•í•œ ì˜ë„
result = result.to('cpu') # ëª…ì‹œì  CPU ì´ë™
```

### 3. ëª¨ë¸ ë¡œë”©

#### ğŸ¤” RIFE/FILM ë°©ì‹: ìºì‹± ì—†ìŒ
```python
# ë§¤ ì‹¤í–‰ë§ˆë‹¤ ë¡œë“œ
model = load_model(model_path)
model.eval().to(device)
# ... ì‚¬ìš© í›„ ìë™ í•´ì œ
```

**ì¥ì **:
- ê°„ë‹¨í•œ êµ¬í˜„
- ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ
- ë‹¤ë¥¸ ëª¨ë¸ê³¼ ì¶©ëŒ ì—†ìŒ

**ë‹¨ì **:
- ë§¤ë²ˆ ë¡œë“œ ì˜¤ë²„í—¤ë“œ (3-5ì´ˆ)
- ë™ì¼ ëª¨ë¸ ì¬ì‚¬ìš© ì‹œ ë¹„íš¨ìœ¨

#### ğŸ¯ ëŒ€ì•ˆ: ê¸€ë¡œë²Œ ìºì‹± (TLBVFI ë°©ì‹)
```python
_MODEL_CACHE = {}

cache_key = f"{model_name}_{gpu_id}"
if cache_key in _MODEL_CACHE:
    model = _MODEL_CACHE[cache_key]
else:
    model = load_model(...)
    _MODEL_CACHE[cache_key] = model
```

**ì¥ì **:
- ì¬ì‚¬ìš© ì‹œ ì¦‰ì‹œ ì‹¤í–‰
- ì›Œí¬í”Œë¡œìš° ê°„ ê³µìœ 

**ë‹¨ì **:
- ë©”ëª¨ë¦¬ ì••ë°• ê´€ë¦¬ í•„ìš”
- ëª…ì‹œì  í´ë¦¬ì–´ ë©”ì»¤ë‹ˆì¦˜ í•„ìš”

### 4. í”„ë ˆì„ ì²˜ë¦¬

#### âœ… DO: ì‚¬ì „ í• ë‹¹ (ì•Œë ¤ì§„ í¬ê¸°)
```python
# RIFE ìŠ¤íƒ€ì¼: í¬ê¸°ë¥¼ ì•Œ ë•Œ
output = torch.zeros(expected_size, device="cpu")
```

#### âœ… DO: ë™ì  ë¦¬ìŠ¤íŠ¸ (ê°€ë³€ í¬ê¸°)
```python
# FILM ìŠ¤íƒ€ì¼: í¬ê¸°ë¥¼ ëª¨ë¥¼ ë•Œ
output_list = []
output_list.extend(frames)
output = torch.cat(output_list)
```

### 5. dtype ê´€ë¦¬

#### âœ… DO: ë©”ëª¨ë¦¬ vs ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„
```python
# RIFE: ë©”ëª¨ë¦¬ ìš°ì„ 
output = torch.zeros(..., dtype=torch.float16)

# FILM: ì •í™•ë„ ìš°ì„ 
output = torch.zeros(..., dtype=torch.float32)
```

---

## TLBVFI êµ¬í˜„ê³¼ì˜ ë¹„êµ

### í˜„ì¬ TLBVFI êµ¬í˜„

```python
# nodes/tlbvfi_interpolator.py
_MODEL_CACHE = {}  # ê¸€ë¡œë²Œ ìºì‹±

def interpolate(self, frame_pair, model_name, times_to_interpolate, gpu_id, ...):
    device = torch.device(f"cuda:{gpu_id}")

    # ìºì‹± ì‚¬ìš©
    cache_key = f"{model_name}_{gpu_id}"
    if cache_key in _MODEL_CACHE:
        model = _MODEL_CACHE[cache_key]
    else:
        # ë©”ëª¨ë¦¬ ì••ë°• ì²´í¬
        if device.type == 'cuda':
            mem_stats = get_memory_stats(device)
            if mem_stats['free'] < 4.0:
                clear_model_cache()

        model = load_tlbvfi_model(model_name, device)
        _MODEL_CACHE[cache_key] = model

    # GPUì—ì„œ ëª¨ë“  ì²˜ë¦¬
    current_frames = [frame1, frame2]
    for iteration in range(times_to_interpolate):
        temp_frames = [current_frames[0]]
        for j in range(len(current_frames) - 1):
            with torch.no_grad():
                mid_frame = model.sample(current_frames[j], current_frames[j+1])
            temp_frames.extend([mid_frame, current_frames[j+1]])
        current_frames = temp_frames

    # í›„ì²˜ë¦¬: CPUë¡œ ì´ë™
    processed_frames = []
    for frame in frames_to_process:
        frame_cpu = frame.squeeze(0).to('cpu', non_blocking=True)
        frame_cpu = (frame_cpu + 1.0) / 2.0
        frame_cpu = frame_cpu.clamp(0, 1)
        frame_cpu = frame_cpu.permute(1, 2, 0)
        processed_frames.append(frame_cpu)

    result = torch.stack(processed_frames, dim=0)

    # ì •ë¦¬
    del current_frames, temp_frames, frame1, frame2
    cleanup_memory(device, force_gc=True)

    return (result,)
```

### ë¹„êµ ë¶„ì„

| ì¸¡ë©´ | TLBVFI | RIFE/FILM |
|------|--------|-----------|
| **ëª¨ë¸ ìºì‹±** | âœ… ê¸€ë¡œë²Œ ìºì‹œ | âŒ ë§¤ë²ˆ ë¡œë“œ |
| **ë©”ëª¨ë¦¬ ì••ë°• ê°ì§€** | âœ… ìë™ ì²´í¬ | âŒ ì—†ìŒ |
| **ì²˜ë¦¬ ìœ„ì¹˜** | GPU (ì¤‘ê°„ í”„ë ˆì„ë“¤) | GPUâ†’CPU ì¦‰ì‹œ ì´ë™ |
| **ì¶œë ¥ ëˆ„ì ** | CPU (ìµœì¢…) | CPU (ì¦‰ì‹œ) |
| **ìºì‹œ í´ë¦¬ì–´** | ìˆ˜ë™ (`force_gc`) | ì£¼ê¸°ì  (10í”„ë ˆì„) |
| **ë¹„ë™ê¸° ì „ì†¡** | âœ… `non_blocking=True` | âŒ ë™ê¸° ì „ì†¡ |
| **ì›Œí¬í”Œë¡œìš° ì¬ì‚¬ìš©** | âœ… ëª¨ë¸ ìœ ì§€ | âŒ ë§¤ë²ˆ ë¡œë“œ |

### ì¥ë‹¨ì 

#### TLBVFI ì¥ì 
- âœ… **ë¹ ë¥¸ ì¬ì‹¤í–‰**: ëª¨ë¸ ìºì‹±ìœ¼ë¡œ ë¡œë“œ ì‹œê°„ ì ˆì•½
- âœ… **ë©”ëª¨ë¦¬ ì•ˆì „**: ìë™ ì••ë°• ê°ì§€ ë° í´ë¦¬ì–´
- âœ… **ë¹„ë™ê¸° ìµœì í™”**: `non_blocking` ì „ì†¡

#### TLBVFI ë‹¨ì  (RIFE/FILM ëŒ€ë¹„)
- âŒ **ì£¼ê¸°ì  ì •ë¦¬ ì—†ìŒ**: í˜ì–´ ë‹¨ìœ„ ì²˜ë¦¬ë¼ ëœ í•„ìš”í•˜ì§€ë§Œ ê³ ë ¤ ê°€ëŠ¥
- âŒ **ComfyUI í†µí•© ë¶€ì¡±**: `soft_empty_cache()` ë¯¸ì‚¬ìš©
- âš ï¸ **GPU ì¤‘ê°„ ëˆ„ì **: ë°˜ë³µ ë³´ê°„ ì‹œ GPU ë©”ëª¨ë¦¬ ì••ë°•

---

## ê¶Œì¥ì‚¬í•­

### 1. ComfyUI í†µí•© ê°œì„ 

**í˜„ì¬**:
```python
cleanup_memory(device, force_gc=True)
```

**ê¶Œì¥**:
```python
from comfy.model_management import soft_empty_cache
soft_empty_cache()  # ComfyUIì˜ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê´€ë¦¬
gc.collect()
```

**ì´ìœ **: ComfyUIì˜ ë‹¤ë¥¸ ë…¸ë“œë“¤ê³¼ í˜‘ì¡°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê´€ë¦¬

### 2. ì£¼ê¸°ì  ìºì‹œ í´ë¦¬ì–´ ê³ ë ¤ (ChunkProcessor)

`TLBVFI_ChunkProcessor`ëŠ” ì—¬ëŸ¬ í˜ì–´ë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ:

```python
# nodes/chunk_processor.py
for pair_idx in tqdm(range(total_pairs)):
    interpolated_frames = self._interpolate_pair(...)
    self._save_chunk_as_video(...)

    # ì¶”ê°€: ì£¼ê¸°ì  ìºì‹œ í´ë¦¬ì–´
    if (pair_idx + 1) % 10 == 0:
        from comfy.model_management import soft_empty_cache
        print(f"Chunk {pair_idx}: Clearing cache...")
        soft_empty_cache()

    cleanup_memory(device, force_gc=True)
```

### 3. ëª¨ë¸ ìºì‹± ì „ëµ ìœ ì§€

TLBVFIì˜ ê¸€ë¡œë²Œ ìºì‹±ì€ **ì¥ì ì´ ë§ìœ¼ë¯€ë¡œ ìœ ì§€** ê¶Œì¥:

```python
_MODEL_CACHE = {}  # ìœ ì§€

# ë©”ëª¨ë¦¬ ì••ë°• ì‹œ ìë™ í´ë¦¬ì–´ (í˜„ì¬ êµ¬í˜„ ìœ ì§€)
if mem_stats['free'] < 4.0:
    clear_model_cache()
```

**ì´ìœ **:
- TLBVFI ëª¨ë¸ì€ 3.6GBë¡œ ë§¤ìš° í¼
- ì¬ë¡œë“œ ì‹œê°„ì´ RIFE/FILMë³´ë‹¤ í›¨ì”¬ ê¹€
- í˜ì–´ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ìºì‹± íš¨ê³¼ ê·¹ëŒ€í™”

### 4. ì‚¬ìš©ì ì„¤ì • ì¶”ê°€ ê³ ë ¤

RIFE/FILMì²˜ëŸ¼ ìºì‹œ í´ë¦¬ì–´ ë¹ˆë„ë¥¼ ì‚¬ìš©ìê°€ ì¡°ì ˆí•  ìˆ˜ ìˆë„ë¡:

```python
class TLBVFI_ChunkProcessor:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                ...
                "clear_cache_after_n_pairs": ("INT", {
                    "default": 10,
                    "min": 1,
                    "max": 100
                }),
            }
        }
```

### 5. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì¶”ê°€

```python
def process_all_chunks(self, ...):
    for pair_idx in range(total_pairs):
        # ì²˜ë¦¬ ì „
        if pair_idx % 50 == 0:  # 50ê°œë§ˆë‹¤ ë¡œê¹…
            mem_stats = get_memory_stats(device)
            print(f"Pair {pair_idx}: VRAM {mem_stats['used']:.1f}GB / {mem_stats['total']:.1f}GB")

        # ì²˜ë¦¬...
```

### 6. dtype ìµœì í™” ê³ ë ¤

í˜„ì¬ TLBVFIëŠ” `float32` ì‚¬ìš©. ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¶œë ¥ì„ `float16`ìœ¼ë¡œ:

```python
# ì²˜ë¦¬ëŠ” float32, ì €ì¥ì€ float16
processed_frames = []
for frame in frames_to_process:
    frame_cpu = frame.squeeze(0).to('cpu', non_blocking=True)
    frame_cpu = (frame_cpu + 1.0) / 2.0
    frame_cpu = frame_cpu.clamp(0, 1).to(dtype=torch.float16)  # float16 ë³€í™˜
    processed_frames.append(frame_cpu.permute(1, 2, 0))
```

---

## ê²°ë¡ 

### í•µì‹¬ ë°œê²¬

1. **RIFE/FILMì€ ì‹¬í”Œí•¨ì„ ì„ íƒ**: ëª¨ë¸ ìºì‹± ì—†ì´ ë§¤ë²ˆ ë¡œë“œ
2. **ì£¼ê¸°ì  ì •ë¦¬ê°€ í•µì‹¬**: `soft_empty_cache()` + `gc.collect()`ë¥¼ 10í”„ë ˆì„ë§ˆë‹¤
3. **CPU ê¸°ë°˜ ì¶œë ¥**: GPU ë©”ëª¨ë¦¬ ì••ë°• ìµœì†Œí™”
4. **ComfyUI í†µí•© ì¤‘ìš”**: `soft_empty_cache()`ë¡œ ë‹¤ë¥¸ ë…¸ë“œì™€ í˜‘ì¡°

### TLBVFIì˜ ì°¨ë³„í™”ëœ ì¥ì 

- âœ… **ëª¨ë¸ ìºì‹±**: 3.6GB ëª¨ë¸ì˜ ì¬ë¡œë“œ ì˜¤ë²„í—¤ë“œ ì œê±°
- âœ… **ë©”ëª¨ë¦¬ ì••ë°• ê°ì§€**: ìë™ ìºì‹œ í´ë¦¬ì–´
- âœ… **í˜ì–´ ë‹¨ìœ„ ì²˜ë¦¬**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì„¤ê³„
- âœ… **ë¹„ë™ê¸° ì „ì†¡**: ì„±ëŠ¥ ìµœì í™”

### ê°œì„  ì œì•ˆ ìš°ì„ ìˆœìœ„

1. **ë†’ìŒ**: ComfyUIì˜ `soft_empty_cache()` í†µí•©
2. **ì¤‘ê°„**: ChunkProcessorì— ì£¼ê¸°ì  ìºì‹œ í´ë¦¬ì–´ ì¶”ê°€
3. **ë‚®ìŒ**: ì‚¬ìš©ì ì„¤ì • ê°€ëŠ¥í•œ ìºì‹œ í´ë¦¬ì–´ ë¹ˆë„
4. **ì„ íƒ**: dtype ìµœì í™” (float16 ì¶œë ¥)

---

## ì°¸ê³ ìë£Œ

- [ComfyUI-Frame-Interpolation Repository](https://github.com/Fannovel16/ComfyUI-Frame-Interpolation)
- [RIFE Paper (ECCV 2022)](https://arxiv.org/abs/2011.06294)
- [FILM Paper (ECCV 2022)](https://arxiv.org/abs/2202.04901)
- [ComfyUI Model Management](https://github.com/comfyanonymous/ComfyUI/blob/master/comfy/model_management.py)

---

**ë¶„ì„ ì™„ë£Œ**: 2025-10-15
**ë‹¤ìŒ ë‹¨ê³„**: ìœ„ ê¶Œì¥ì‚¬í•­ ì¤‘ ë†’ì€ ìš°ì„ ìˆœìœ„ í•­ëª©ë¶€í„° êµ¬í˜„ ê²€í† 
